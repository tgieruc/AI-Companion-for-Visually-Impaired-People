# AI Companion for Visually Impaired People

This project was created during the [GenAI Hack AI Campus](https://www.merantix-aicampus.com/event/hack-ai-campus-genai-edition-2023). The goal was to create a prototype that helps visually impaired people to navigate in their environment. 

The user verbally asks a question about their environment, a photo is taken and AI companion answers the question using text-to-speech, leveraging VQA and Object Detection. 

The prototype is based on the [Google Cloud Platform](https://cloud.google.com/?hl=en) and uses [Palm2](https://ai.google/discover/palm2/), [VILT VQA](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/25?project=our-forest-401807), [BLIP-2](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/48?project=our-forest-401807), [YOLOv8](https://github.com/ultralytics/ultralytics), [text-to-speech](https://console.cloud.google.com/vertex-ai/generative/speech/text-to-speech?project=our-forest-401807) and [speech-to-text](https://console.cloud.google.com/vertex-ai/generative/speech/speech-to-text?project=our-forest-401807).


## Team
* Aditi Bhalla
* Mert Keser
* Th√©o Gieruc
* Wencan Huang